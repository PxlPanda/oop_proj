import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image

from recognition.crnn_model import CRNN
from recognition.utils_ctc import collate_fn

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏
DATASET_DIR = "dataset"
EPOCHS = 1
BATCH_SIZE = 16
NORMALIZE_CASE = False
MAX_SAMPLES = 5000  # None –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤

# === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ TSV
def load_tsv(tsv_path, image_folder):
    samples = []
    if not os.path.exists(tsv_path):
        return samples
    with open(tsv_path, encoding="utf-8") as f:
        for line in f:
            parts = line.strip().split("\t")
            if len(parts) != 2:
                continue
            filename, label = parts
            if NORMALIZE_CASE:
                label = label.lower()
            img_path = os.path.join(image_folder, filename)
            if os.path.exists(img_path):
                samples.append((img_path, label))
    return samples

# === –°–±–æ—Ä –≤—Å–µ—Ö —Å—ç–º–ø–ª–æ–≤
samples = []

# 1) –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
for letter_dir in sorted(os.listdir(DATASET_DIR)):
    abs_letter_path = os.path.join(DATASET_DIR, letter_dir)
    if not os.path.isdir(abs_letter_path):
        continue
    if len(letter_dir) != 1 or not letter_dir.isalpha():
        continue
    for fname in os.listdir(abs_letter_path):
        if fname.lower().endswith((".jpg", ".png", ".jpeg")):
            label = letter_dir
            if NORMALIZE_CASE:
                label = label.lower()
            samples.append((os.path.join(abs_letter_path, fname), label))

# 2) —Å–ª–æ–≤–∞ –∏–∑ TSV
samples += load_tsv(os.path.join(DATASET_DIR, "train.tsv"), os.path.join(DATASET_DIR, "train"))
samples += load_tsv(os.path.join(DATASET_DIR, "test.tsv"), os.path.join(DATASET_DIR, "test"))

# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏
if MAX_SAMPLES:
    samples = samples[:MAX_SAMPLES]

# === –ê–ª—Ñ–∞–≤–∏—Ç –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
# Debug: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤
print(f"üì¶ Total samples collected: {len(samples)}")

alphabet = sorted(set(c for _, label in samples for c in label))
alphabet = sorted(set(c for _, label in samples for c in label))
print(f"üî§ Alphabet used for training: {''.join(alphabet)}")
print(f"üî§ Number of characters in alphabet: {len(alphabet)}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º alphabet.txt –≤–º–µ—Å—Ç–µ —Å –º–æ–¥–µ–ª—å—é
weights_dir = "backend/recognition/weights"
os.makedirs(weights_dir, exist_ok=True)
alphabet_path = os.path.join(weights_dir, "alphabet.txt")
with open(alphabet_path, "w", encoding="utf-8") as f:
    f.write("".join(alphabet))
print(f"‚úÖ Saved alphabet to: {alphabet_path}")

char_to_idx = {c: i for i, c in enumerate(alphabet)}
nclass = len(alphabet) + 1
print(f"üî† Computed nclass (alphabet + blank): {nclass}")

# === –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
transform = transforms.Compose([
    transforms.Resize((32, 100)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# === Dataset –∏ DataLoader
class OcrDataset(Dataset):
    def __init__(self, samples, transform):
        self.samples = samples
        self.transform = transform

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        img = Image.open(path).convert("L")
        img = self.transform(img)
        return img, label

dataset = OcrDataset(samples, transform)
loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,
                    collate_fn=lambda b: collate_fn(b, char_to_idx))

# === –ú–æ–¥–µ–ª—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = CRNN(32, 1, nclass, 256).to(device)
criterion = nn.CTCLoss(blank=nclass - 1, zero_infinity=True)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# === –û–±—É—á–µ–Ω–∏–µ (–≤—Å–µ–≥–¥–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—è —Å—Ç–∞—Ä—ã–µ –≤–µ—Å–∞)
print("üöÄ Starting training (old weights will be overwritten)")
for epoch in range(1, EPOCHS + 1):
    model.train()
    total_loss = 0
    start = time.time()
    for i, (images, targets, lengths, labels) in enumerate(loader, 1):
        images = images.to(device)
        targets = targets.to(device)
        preds = model(images)
        preds_log = F.log_softmax(preds, dim=2)
        input_lengths = torch.full((images.size(0),), preds.size(0), dtype=torch.long)
        loss = criterion(preds_log, targets, input_lengths, lengths)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        if i % 20 == 0:
            print(f"Epoch {epoch} Batch {i}/{len(loader)} ‚Äî Loss: {loss.item():.4f}")
    end = time.time()
    print(f"Epoch {epoch}/{EPOCHS} ‚Äî Total Loss: {total_loss:.4f} ‚Äî ‚è± {end - start:.2f}s")

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
checkpoint_path = os.path.join(weights_dir, "crnn_weights.pth")
# –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ —Ç–æ–ª—å–∫–æ –≤–µ—Å–∞, –Ω–æ –∏ –∞–ª—Ñ–∞–≤–∏—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
torch.save({
    'model_state_dict': model.state_dict(),
    'alphabet': ''.join(alphabet)
}, checkpoint_path)
print(f"üíæ Saved model weights and alphabet to: {checkpoint_path}")

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ä–∞–∑—É –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
checkpoint = torch.load(checkpoint_path, map_location=device)
saved_alphabet = checkpoint.get('alphabet', None)
if saved_alphabet is None:
    raise KeyError("‚ùå –í checkpoint –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á 'alphabet'.")
print(f"üî§ Alphabet saved in checkpoint: {saved_alphabet}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ embedding —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç nclass
nclass_from_weights = checkpoint['model_state_dict']['rnn.1.embedding.weight'].shape[0]
print(f"üß† Weights expect nclass (including blank): {nclass_from_weights}")
assert nclass_from_weights == nclass, \
    f"Mismatch: nclass defined={nclass}, weights expect={nclass_from_weights}" + \
    " ‚Äî –≤–æ–∑–º–æ–∂–Ω–æ, alphabet –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤–µ—Å–∞–º."
print("üéâ Training complete and verified.")
